# -*- coding: utf-8 -*-
"""ChatbotFilms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vxQ_WPSnrCdwWdTKV0j4i0Um1AQoE3uZ

# Chatbot de recommandation de films

Recommander des films selon un film référence ou selon une description.
Dans ce projet, vous devez utiliser un LLM open source pour la partie conversationnelle
"""

from google.colab import drive
import pandas as pd

#drive.mount('/content/gdrive')

dataset = pd.read_csv("./mpst_full_data.csv")

"""## Preprocessing pour le synopsis"""

!python -m spacy download en_core_web_sm

"""On tokenise les synopsis des films en utilisant deux tokeniseurs : treebank et spacy"""

from nltk.tokenize import TreebankWordTokenizer
import spacy
from tqdm import tqdm
tqdm.pandas()

nlp = spacy.load("en_core_web_sm")
tokenizer = nlp.tokenizer

def treebank_tokenize(text):
  # fonction qui tokenize avec le tokenizer TreeBank
  return(TreebankWordTokenizer().tokenize(text))

def spacy_tokenize(text):
  # fonction qui tokenize avec le tokenizer spacy
  text_tokenized = tokenizer(text)
  return [token.text for token in text_tokenized]

dataset["synopsis_treebank"] = dataset['plot_synopsis'].progress_apply(treebank_tokenize)

dataset["synopsis_spacy"] = dataset['plot_synopsis'].progress_apply(spacy_tokenize)

"""On effectue également le preprocessing suivant :
* Mettre le texte en minuscule
* Supprimer les stopwords (the, these, a, is, are, ...)  
* Ne garder que les tokens de plus de 2 caractères  
* Ne garder que les tokens qui sont alphabétiques
"""

from spacy.lang.en.stop_words import STOP_WORDS

def preprocess(tokens):
    doc = nlp(' '.join(tokens))

    processed_tokens = [token.text.lower() for token in doc
                        if token.text.lower() not in STOP_WORDS and len(token.text) > 2 and token.is_alpha]

    return processed_tokens

dataset["synopsis_preprocessed_spacy"] = dataset["synopsis_spacy"].progress_apply(preprocess)

dataset["synopsis_preprocessed_treebank"] = dataset["synopsis_treebank"].progress_apply(preprocess)

"""## Preprocessing pour les tags"""

dataset.head()

"""Pour faciliter l'apprentissage pour la prédiction, on pourrait split la colonne tags en plusieurs colonnes."""

dataset['tags_list'] = dataset['tags'].apply(lambda x: x.split(','))
dataset.head()

from itertools import chain

# récupérer tous les tags sous forme de liste
all_tags = list(dataset['tags_list'])

# applatir la liste
all_tags = list(chain(*all_tags))

# supprimer les espaces
all_tags = [element.replace(" ", "") for element in all_tags]

# garder les valeurs distinctes
all_tags = [x for i, x in enumerate(all_tags) if x not in all_tags[:i]]

print("Nombre de tags différents : ", len(all_tags))
print(all_tags)

# On crée une colonne par tag
for tag in all_tags:
  dataset[tag] = dataset['tags'].apply(lambda x: 1 if x.find(tag) != -1 else 0)

dataset.head()

"""## Data exploration

Dans ce jeu de données nous avons différentes variables :
- l'**identifiant** du film
- le **titre du film**
- le **synopsis du film**
- les **tags** que nous avons séparé en plusieurs colonnes
- le **split** de train, test, validation
- la **source** de donnée du synopsis

Pour la tâche de recomandation, nous allons utiliser le **synopsis** et les **tags**
"""

valeurs_nan = dataset.isna().sum()

# Colonnes avec des valeurs NaN
colonnes_avec_nan = valeurs_nan[valeurs_nan > 0]
print("Colonnes avec des valeurs NaN :")
print(colonnes_avec_nan)

"""Il n'y a pas de valeurs non définies dans les données source.

Regardons s'il y a des films qui ont le même titre :
"""

doublons_dans_colonne = dataset['title'].duplicated()

# Affichez les lignes avec des doublons dans la colonne spécifiée
print("Films avec des doublons dans la colonne {}: ".format('title'))
print(dataset['title'][doublons_dans_colonne])

# Utilisez la méthode sum() pour obtenir le nombre total de doublons dans la colonne
nombre_de_doublons = doublons_dans_colonne.sum()

# Affichez le nombre de doublons dans la colonne spécifiée
print("Nombre de doublons dans la colonne {}: {}".format('title', nombre_de_doublons))

"""Regardons d'où proviennent les synopsis :"""

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 5))
dataset['synopsis_source'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title(f'Diagramme circulaire de la colonne synopsis_source')
plt.ylabel('')  # Supprimer le label y
plt.show()

"""Regardons combien il y a de tags au total et quels sont les plus fréquents :"""

colonnes_tags = dataset.columns[dataset.columns.get_loc('tags_list') + 1:]

print('Il y a ', len(colonnes_tags), ' tags différents')

# Somme des valeurs pour chaque colonne
somme_par_colonne = dataset[colonnes_tags].sum(axis=0)

top_10_colonnes = somme_par_colonne.nlargest(10)

plt.figure(figsize=(6, 4))
bars = plt.bar(top_10_colonnes.index, top_10_colonnes, color='skyblue')

# Ajoutez les annotations pour afficher les valeurs sur chaque barre
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, round(yval, 2), ha='center', va='bottom')

plt.xticks(rotation=90)
plt.title('Top 10 Tags les plus présents')
plt.xlabel('Colonnes')
plt.ylabel('Nombre de films avec le tag')
plt.show()

"""## Ajout de la date de sortie du film pour différencier les films ayant le même titre"""

import requests
from bs4 import BeautifulSoup
from time import sleep
from random import randint

tmp = []
iterList = 0

for index, row in dataset.iterrows():

    id = row['imdb_id']
    title = row['title']

    if doublons_dans_colonne[iterList]:

        print(title)
        print("id : " + str(iterList))

        # URL de la page web que vous souhaitez extraire
        url = "https://www.imdb.com/title/"+str(id)

        # Envoi d'une requête pour récupérer le contenu de la page
        response = requests.get(url)

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers)


        # Vérification du statut de la requête
        if response.status_code == 200:

            # Analyse du HTML avec BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')

            # Recherche de tous les éléments <a> avec la classe "ipc-link" et d'autres attributs spécifiques
            elements = soup.find_all('a', class_='ipc-link ipc-link--baseAlt ipc-link--inherit-color')

            tmpElem = None
            # Affichage du contenu de chaque élément extrait
            for element in elements:
                if "releaseinfo" in element['href']:
                    tmpElem = int(element.text[:4])
                    print("year : " + element.text[:4])     # Affiche le texte à l'intérieur de la balise <a>
            if tmpElem == None:
                print(id+"here here here here here here here here here here here here here here here here here here ")
            tmp.append(tmpElem)

        else:
            print(f"Erreur de requête: {response.status_code}")
            tmp.append(None)
            print(0)
    else:
        tmp.append(None)

    iterList += 1

import csv



# Spécifiez le chemin du fichier CSV
#csv_file_path = '/content/gdrive/MyDrive/ING3/NLP/Project/listTemp.csv'

# Écrivez la liste dans le fichier CSV
#with open(csv_file_path, 'w', newline='') as csv_file:
 #   csv_writer = csv.writer(csv_file)
  #  csv_writer.writerow(tmp)

#dataset['Year'] = ""

#iterMovie = 0
#iterDoublon = 0
#print(len(tmp))

#for i in dataset['Year']:
 # if doublons_dans_colonne[iterMovie]:
  #  print(iterMovie)
   # print(iterDoublon)
    #i = tmp[iterDoublon]
  #  print(i)
   # iterDoublon += 1
  #iterMovie += 1

"""## Sauvegarde du dataframe preprocessé"""

#dataset.head()

# En local
#dataset.to_csv('films_dataset.csv', index=False)

# Sur google drive
#dataset.to_csv('./films_dataset.csv', index=False)

"""## NLP"""

from google.colab import drive
import pandas as pd

# Chargement du dataframe après preprocessing

dataset = pd.read_csv("./films_dataset.csv")

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity

"""**Méthode 1**"""

smd1 =  dataset
smd1['tags'] = smd1['tags'].fillna('')
smd1['plot_synopsis'] = smd1['synopsis_preprocessed_spacy'] + smd1['tags']
smd1['plot_synopsis'] = smd1['plot_synopsis'].fillna('')

tf1 = TfidfVectorizer(ngram_range=(1, 2), min_df=0)
tfidf1_matrix = tf1.fit_transform(smd1['plot_synopsis'])

tfidf1_matrix.shape

"""**Méthode 2**"""

smd2 =  dataset
smd2['tags'] = smd2['tags'].fillna('')
smd2['plot_synopsis'] = smd2['synopsis_preprocessed_treebank'] + smd2['tags']
smd2['plot_synopsis'] = smd2['plot_synopsis'].fillna('')

tf2 = TfidfVectorizer(ngram_range=(1, 2), min_df=0)
tfidf2_matrix = tf2.fit_transform(smd2['plot_synopsis'])

tfidf2_matrix.shape

"""**Calcul de la distance avec la similarité du cosinus pour le synopsis**

"""

cosine_sim1 = linear_kernel(tfidf1_matrix, tfidf1_matrix)
cosine_sim2 = linear_kernel(tfidf2_matrix, tfidf2_matrix)

smd = smd1.reset_index()
titles = smd['title']
indices = pd.Series(smd.index, index=smd['title'])

def get_recommendations(cosine_sim, title):
    idx = indices[title]

    sim_scores = list(enumerate(cosine_sim[idx]))
    if isinstance(sim_scores[0][1], float):
      sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
      x=1
    else:
      idx = idx[0]
      sim_scores = list(enumerate(cosine_sim[idx]))
      sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
      x=0
    sim_scores = sim_scores[x:31]
    movie_indices = [i[0] for i in sim_scores]

    selected_movies = smd.iloc[movie_indices].apply(lambda x: {'title': x['title'], 'year': int(x['year'])} if pd.notna(x['year']) else {'title': x['title']}, axis=1).head(10)
    selected_movies_list = selected_movies.tolist()

    return selected_movies_list

get_recommendations(cosine_sim1, 'Dungeons & Dragons')

smd = smd2.reset_index()
titles = smd['title']
indices = pd.Series(smd.index, index=smd['title'])

get_recommendations(cosine_sim2, 'Dungeons & Dragons')

"""## LLM

Nous avons choisi de travailler avec GPT-2 et Flan-T5 pour la partie conversationnelle. Ainsi, on le laisse discuter avec l'utilisateur, et lorsqu'un mot clé du champ lexical des films ou de recommandation apparaît, on récupère les tags et le synopsis et on renvoie une liste de films correspondant à la demande de l'utilisateur.

Avec GPT-2 :
"""

# Installation des bibliothèques
!pip install transformers

# Importation des modules
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Chargement du modèle et du tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token_id = tokenizer.eos_token_id
model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)


# Fonction pour générer du texte
def generate_response(prompt, model, tokenizer, max_length=50, num_beams=20, no_repeat_ngram_size=2, top_k=50, top_p=1, temperature=0.5):
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    attention_mask = inputs.ne(tokenizer.pad_token_id).float()
    outputs = model.generate(inputs, max_length=max_length, num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size, top_k=top_k, top_p=top_p, temperature=temperature, do_sample=True, attention_mask=attention_mask)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated_text = generated_text.split("\n")[1:]
    generated_text = "".join(generated_text)
    return generated_text

# Début de la discussion
user_input = ""
while True:
    user_input = input("Entrez votre message : ")

    # Pour terminer le chat
    if "exit" in user_input.lower() or "bye" in user_input.lower():
        print("Bot: Bye.")
        break

    # Recomendation de film
    if ("recommend" in user_input.lower() or "give" in user_input.lower()) and ("film" in user_input.lower() or "movie" in user_input.lower()):
            response = "Here is a list of movies you could like, based on your preferences: "
            film = user_input.split("'")[1]
            if len(dataset[dataset['title'] == film] != 0):
              films = get_recommendations(cosine_sim1, film)
              res = ""
              for film in films :
                if 'year' in film:
                  if film == films[len(films)-1]:
                    res+=film['title']+" ("+str(film['year'])+")"
                  else:
                    res=res+film['title']+" ("+str(film['year'])+"), "
                else:
                  if film == films[len(films)-1]:
                    res+=film['title']
                  else:
                    res=res+film['title']+", "
              response = response+res
            else:
              response = "Sorry we don't have " + film + " in our database."
    else:
      response = generate_response(user_input, model, tokenizer)
    print("Bot:", response)

"""Avec Flan-T5 :"""

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Import du modèle Flan-T5
flan_t5_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
flan_t5_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

def post_process_flan_t5(response):
    # Supprimez les tokens spéciaux et les avertissements du modèle
    cleaned_response = response.replace("<pad>", "").replace("</s>", "").strip()
    return cleaned_response

while True:
    user_input = input("Entrez votre message : ")

    # Pour terminer le chat
    if "exit" in user_input.lower() or "bye" in user_input.lower():
        print("Bot: Bye.")
        break

    # Recomendation de film
    if ("recommend" in user_input.lower() or "give" in user_input.lower()) and ("film" in user_input.lower() or "movie" in user_input.lower()):
            response = "Here is a list of movies you could like, based on your preferences: "
            film = user_input.split("'")[1]
            if len(dataset[dataset['title'] == film] != 0):
              films = get_recommendations(cosine_sim2, film)
              res = ""
              for film in films :
                if 'year' in film:
                  if film == films[len(films)-1]:
                    res+=film['title']+" ("+str(film['year'])+")"
                  else:
                    res=res+film['title']+" ("+str(film['year'])+"), "
                else:
                  if film == films[len(films)-1]:
                    res+=film['title']
                  else:
                    res=res+film['title']+", "
              response = response+res
            else:
              response = "Sorry we don't have " + film + " in our database."
    # Réponse normale du modèle
    else:
        inputs = flan_t5_tokenizer(user_input, return_tensors="pt")
        outputs = flan_t5_model.generate(**inputs)
        response = post_process_flan_t5(flan_t5_tokenizer.decode(outputs[0]))

    print("Bot:", response)

# Test :
# Can you recommend me a film similar to 'The Shop Around the Corner'?
